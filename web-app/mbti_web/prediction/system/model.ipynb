{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update sklearn\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding, BatchNormalization, Activation, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2 \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sn\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import joblib\n",
    "try: \n",
    "    import nlp\n",
    "except Exception:\n",
    "    from . import nlp\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import argmax\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('MBTI_Clean_test.csv')\n",
    "#data.head()\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, tokenizer = None):\n",
    "    print(\"Now preproccessing\")\n",
    "    print(\"Renaming columns\")\n",
    "    print(data.head())\n",
    "    print(\"Data Dropped\")\n",
    "\n",
    "    tokens = nlp.split_words(data.posts)\n",
    "    print(\"Data are tokens now\")\n",
    "\n",
    "    data['tokens'] = tokens\n",
    "\n",
    "    print(data.head())\n",
    "    sentence_lengths = [len(tokens) for tokens in data[\"tokens\"]]\n",
    "    print(\"Max sentence length is %s\" % max(sentence_lengths))\n",
    "    data['sentence_lengths'] = sentence_lengths\n",
    "    print(\"about to lemmatize\")\n",
    "    lemmatized_posts = nlp.lemmatize(tokens)\n",
    "    data['lemmatized_posts'] = lemmatized_posts\n",
    "    print(data.head())\n",
    "    if tokenizer == None:\n",
    "        tokenizer = nlp.create_tokenizer(data)\n",
    "    return data, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    print(\"In spilt Data\")\n",
    "    y = data[\"binarized_target\"].values\n",
    "    print(y)\n",
    "    try:\n",
    "        data_train, data_test, y_train, y_test = train_test_split(data, y, test_size=0.10, random_state=500, stratify=y)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    print(\"about to return\")\n",
    "    return data_train, data_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cnn_data(data, data_train, data_test, tokenizer):\n",
    "    MAX_SEQUENCE_LENGTH = 1200\n",
    "    train_cnn_data = nlp.tokenize(data_train, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "    test_cnn_data = nlp.tokenize(data_test, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "    train_word_index = tokenizer.word_index\n",
    "    return train_cnn_data, test_cnn_data, train_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNModel(embedding_matrix, max_sequence_length, num_words, embedding_dim):\n",
    "    inputs = Input(shape=(max_sequence_length, ))\n",
    "    embedding_layer = Embedding(num_words,embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=True)(inputs)\n",
    "    convs = []\n",
    "    filter_sizes = [1,2,3,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=256, kernel_size=filter_size, activation='relu')(embedding_layer)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    y = Dense(128, activation='relu')(l_merge)\n",
    "    y = Dropout(0.1)(y)  \n",
    "    preds = Dense(1, activation='sigmoid')(l_merge)\n",
    "    model = Model(inputs, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrain(data):\n",
    "    data , tokenizer = preprocess_data(data)\n",
    "    embeddings_index = {}\n",
    "    try:\n",
    "#        f = codecs.open('./data/wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "        f = codecs.open('./prediction/system/data/wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "        print(\"opened the file\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    for line in tqdm(f):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('found %s word vectors' % len(embeddings_index))\n",
    "    return data, embeddings_index, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(data, tokenizer, embeddings_index, character):\n",
    "    MAX_SEQUENCE_LENGTH = 1200\n",
    "    EMBEDDING_DIM = 300\n",
    "    data = nlp.split_target_variable(data, character)\n",
    "    print(\"split of targets done\")\n",
    "    data = nlp.binarize_target_variable(data)\n",
    "    print(\"binarizetion of targets \")\n",
    "    # data['sentence_lengths'].hist(bins = 30)\n",
    "    undersampled_data = nlp.undersample(data)\n",
    "    print(\"undersampled data is done\")\n",
    "    print(undersampled_data.head())\n",
    "    X_train, X_test, y_train, y_test = split_data(undersampled_data)\n",
    "    print(\"dataset Splitting is done\")\n",
    "    cnn_train, cnn_test, word_index = prepare_cnn_data(data, X_train, X_test, tokenizer)\n",
    "    \n",
    "    print('preparing embedding matrix...')\n",
    "    words_not_found = []\n",
    "    nb_words = min(999999, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            words_not_found.append(word)\n",
    "    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))    \n",
    "    \n",
    "    filepath='./prediction/system/data/weights' +'_'+ str(character) + '.best.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    model = CNNModel(embedding_matrix, MAX_SEQUENCE_LENGTH, len(word_index)+1, EMBEDDING_DIM)\n",
    "    hist = model.fit(cnn_train, y_train, epochs=4, validation_data=(cnn_test, y_test), shuffle=True, callbacks=callbacks_list)\n",
    "    filename = './prediction/system/data/model' +'_'+ str(character) + '.sav'\n",
    "    joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAllModel(data):\n",
    "    prepared_data, embeddings_index, tokenizer = prepareTrain(data)\n",
    "    print(\"Data has been prepaired\")\n",
    "    trainModel(prepared_data, tokenizer, embeddings_index, 0)\n",
    "    trainModel(prepared_data, tokenizer, embeddings_index, 1)\n",
    "    trainModel(prepared_data, tokenizer, embeddings_index, 2)\n",
    "    trainModel(prepared_data, tokenizer, embeddings_index, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainAllModel(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateNewModel(data, character, model, tokenizer):\n",
    "    MAX_SEQUENCE_LENGTH = 1200\n",
    "    \n",
    "    data, tokenizer = preprocess_data(data, tokenizer)\n",
    "    data = nlp.split_target_variable(data, character)\n",
    "    print(\"split of targets done\")\n",
    "    data = nlp.binarize_target_variable(data)\n",
    "    print(\"binarizetion of targets \")\n",
    "    y = data[\"binarized_target\"].values\n",
    "    X = nlp.tokenize(data, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "    y_pred = model.predict(X, verbose=1)\n",
    "    \n",
    "    y_pred_bool = [1 * (x[0]>=0.5) for x in y_pred]\n",
    "\n",
    "\n",
    "    return classification_report(y, y_pred_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluateModel(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tabular_data(data, data_train, data_test, y_train):\n",
    "    tabular_train_data, tabular_test_data = nlp.tabular_features(data, data_train, data_test)\n",
    "    tabular_train_data, tabular_test_data = nlp.tabular_scaler(tabular_train_data, tabular_test_data)\n",
    "    tabular_train_best_data, tabular_test_best_data = nlp.chi2_features(tabular_train_data, tabular_test_data, y_train)\n",
    "    return tabular_train_best_data, tabular_test_best_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_model(embedding_matrix, max_sequence_length, num_words, embedding_dim):\n",
    "    \n",
    "    inputA = Input(shape=(100,))\n",
    "    inputB = Input(shape=(max_sequence_length, ))\n",
    "    \n",
    "    # Tabular data branch\n",
    "    x = Dense(2, activation=\"relu\")(inputA)\n",
    "    x = Model(inputs=inputA, outputs=x)   \n",
    "    \n",
    "    \n",
    "    # CNN data branch\n",
    "    embedding_layer = Embedding(num_words,embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=True)(inputB)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [1,2,3,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=256, kernel_size=filter_size, activation='relu')(embedding_layer)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    y = Dense(128, activation='relu')(l_merge)\n",
    "    y = Dropout(0.1)(y)  \n",
    "    y = Dense(2, activation=\"relu\")(y)\n",
    "    y = Model(inputs=inputB, outputs=y)\n",
    "    \n",
    "    combined = concatenate([x.output, y.output])\n",
    "    \n",
    "    z = BatchNormalization()(combined)\n",
    "    \n",
    "\n",
    "    preds = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs = [x.input, y.input], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel():\n",
    "    inputA = Input(shape=(100,))    \n",
    "    preds = Dense(1, activation=\"sigmoid\")(inputA)\n",
    "    model = Model(inputA, preds)\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
