{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from bert_embedding import BertEmbedding\n",
    "from nltk.corpus import wordnet\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2 \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the .csv file\n",
    "#df = pd.read_csv(\"mbti.csv\")\n",
    "#posts = df.posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split words\n",
    "def split_words(posts):\n",
    "    tokenized_posts = []\n",
    "    for row in posts:\n",
    "        token = word_tokenize(row)\n",
    "        if token != '':\n",
    "            tokenized_posts.append(token)\n",
    "#    print('tokenized posts', tokenized_posts)\n",
    "    return tokenized_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_num(posts):\n",
    "    return len(tokenize(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokenized_posts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_posts = []\n",
    "    for sentence in tokenized_posts:\n",
    "        tagged = pos_tag(sentence)\n",
    "        lemmatized_sentence = []\n",
    "        for word, tag in tagged:\n",
    "            wntag = get_wordnet_pos(tag)\n",
    "            if wntag is None:\n",
    "                lemmatized_sentence.append(word)\n",
    "            else:\n",
    "                lemmatized_sentence.append(lemmatizer.lemmatize(word, pos=wntag))\n",
    "        lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "        lemmatized_posts.append(lemmatized_sentence)\n",
    "    return lemmatized_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(data):\n",
    "    all_words = [word for tokens in data[\"tokens\"] for word in tokens]\n",
    "    VOCAB = sorted(list(set(all_words)))\n",
    "    tokenizer = Tokenizer(num_words=len(VOCAB), lower=True, char_level=False)\n",
    "    \n",
    "    with open('tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data, tokenizer, sequence_length):\n",
    "    tokenizer.fit_on_texts(data[\"lemmatized_posts\"])\n",
    "    sequences = tokenizer.texts_to_sequences(data[\"lemmatized_posts\"])\n",
    "    cnn_data = pad_sequences(sequences, maxlen=sequence_length)\n",
    "    return cnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(tokenizer):\n",
    "    vocab_size = len(tokenizer.word_index) + 1  \n",
    "    return vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove_file = \"glove.6B.50d.txt\"\n",
    "#vocab_size = get_vocab_size()\n",
    "#dim = 50\n",
    "\n",
    "def get_embedding_matrix(dim, vocab_size, glove_file, word_index):\n",
    "    embedding_matrix = np.zeros((vocab_size, dim))\n",
    "    with open( glove_file , encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_target_variable(data, character):\n",
    "    data['character'] = [characters[character] for characters in data.type]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_target_variable(data):\n",
    "    data['binarized_target'] = data.character.astype('category').cat.codes\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(data):\n",
    "    major = 0\n",
    "    if data.groupby('binarized_target').count().sort_values('posts').index[-1] == 1:\n",
    "        major = 1\n",
    "    minor = 1 - major\n",
    "    no_minor = len(data[data['binarized_target'] == minor])\n",
    "    \n",
    "    major_indices = data[data.binarized_target == major].index\n",
    "    random_indices = np.random.choice(major_indices,no_minor, replace=False)\n",
    "    minor_indices = data[data.binarized_target == minor].index\n",
    "    \n",
    "    under_sample_indices = np.concatenate([minor_indices,random_indices])\n",
    "    under_sample = data.loc[under_sample_indices]\n",
    "    return under_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_features(tabular_data, tabular_train, tabular_test):\n",
    "    tabular_train_data = pd.DataFrame()\n",
    "    tabular_test_data = pd.DataFrame()\n",
    "    for i in np.arange(1,4):\n",
    "        tfidf = TfidfVectorizer(stop_words='english',ngram_range=(i,i), decode_error='replace', max_features = 100000)\n",
    "        tabular_word_data = tfidf.fit(tabular_data['posts'].values.astype('U'))\n",
    "        tabular_word_train = tfidf.fit_transform(tabular_train['posts'].values.astype('U'))\n",
    "        tabular_word_test = tfidf.transform(tabular_test['posts'].values.astype('U'))\n",
    "\n",
    "\n",
    "        tsvd = TruncatedSVD(n_components=500, algorithm='arpack', random_state=500)\n",
    "        tabular_wordie_train = tsvd.fit_transform(tabular_word_train)\n",
    "        tabular_wordie_test = tsvd.transform(tabular_word_test)\n",
    "        tabular_wordie_train_df = pd.DataFrame(tabular_wordie_train,\n",
    "                                        columns=[str(i)+'_'+str(b) for b in np.arange(1,tabular_wordie_train.shape[1]+1)])\n",
    "        tabular_wordie_test_df = pd.DataFrame(tabular_wordie_test,\n",
    "                                       columns=[str(i)+'_'+str(b) for b in np.arange(1,tabular_wordie_test.shape[1]+1)])\n",
    "        tabular_train_data = pd.concat([tabular_train_data,tabular_wordie_train_df], axis=1)\n",
    "        tabular_test_data = pd.concat([tabular_test_data,tabular_wordie_test_df], axis=1)\n",
    "    return tabular_train_data, tabular_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_scaler(tabular_train_data, tabular_test_data):\n",
    "    scaler = MinMaxScaler()\n",
    "    tabular_train_data = pd.DataFrame(scaler.fit_transform(tabular_train_data),\n",
    "                                      columns=tabular_train_data.columns, index=tabular_train_data.index)\n",
    "    tabular_test_data = pd.DataFrame(scaler.transform(tabular_test_data),\n",
    "                                      columns=tabular_test_data.columns, index=tabular_test_data.index)\n",
    "    return tabular_train_data, tabular_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_features(tabular_train_data, tabular_test_data, tabular_y_train):\n",
    "    chi2_features = SelectKBest(chi2, k = 100) \n",
    "    tabular_train_best_data = pd.DataFrame(chi2_features.fit_transform(tabular_train_data, tabular_y_train))\n",
    "    tabular_test_best_data = pd.DataFrame(chi2_features.transform(tabular_test_data))\n",
    "    return tabular_train_best_data, tabular_test_best_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the % of the vocabulary covered by the pretrained model\n",
    "#nonzero_elements = np.count_nonzero(np.count_nonzero(get_embedding_matrix(50, get_vocab_size(posts),\"glove.6B.50d.txt\" ), axis=1))\n",
    "#nonzero_elements / get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
