{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import joblib\n",
    "import nlp\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import argmax\n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePrediction(input):\n",
    "    model_load_0 = joblib.load('model_0.sav')\n",
    "    model_load_1 = joblib.load('model_1.sav')\n",
    "    model_load_2 = joblib.load('model_2.sav')\n",
    "    model_load_3 = joblib.load('model_3.sav')\n",
    "    model = load_model('weights.best.hdf5')\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    data = pd.DataFrame(columns = ['input', 'lemmatized_posts'])\n",
    "    data['input'] = [input]\n",
    "    tokens = nlp.split_words(data)\n",
    "    lemmatized_input = nlp.lemmatize(tokens)\n",
    "    data['lemmatized_posts'] = [lemmatized_input]\n",
    "    input_data = nlp.tokenize(data, tokenizer, 1200)\n",
    "    prediction_0 = model_load_0.predict(input_cnn_data)\n",
    "    prediction_1 = model_load_1.predict(input_cnn_data)\n",
    "    prediction_2 = model_load_2.predict(input_cnn_data)\n",
    "    prediction_3 = model_load_3.predict(input_cnn_data)\n",
    "    return prediction_0, prediction_1, prediction_2, prediction_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_0, prediction_1, prediction_2, prediction_3 = makePrediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2093962]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction_3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_load_0 = joblib.load('model_0.sav')\n",
    "model_load_1 = joblib.load('model_1.sav')\n",
    "model_load_2 = joblib.load('model_2.sav')\n",
    "model_load_3 = joblib.load('model_3.sav')\n",
    "model = load_model('weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input = \"If an offer has a time limit, I always pass the first time around. If it really is quality content, it will come back around. I did this a couple of years ago with a writing class I badly wanted to take! I did my research and replied to the nurturing emails I received to interact with the woman in charge. When it came time to “sign up in the next week before the window closes” —I didn’t. I reigned in my enthusiasm and let the time frame pass. I continued to follow the posts and emails about the content for the next few months. When another window opened up for students, I felt even better about signing up! It was totally worth the wait to make sure I wasn’t tossing my money at something shiny. \"\n",
    "data = [input]\n",
    "tokens = nlp.split_words(data)\n",
    "lemmatized_posts = nlp.lemmatize(tokens)\n",
    "input_sequences = tokenizer.texts_to_sequences(lemmatized_posts)\n",
    "print(input_sequences)\n",
    "input_cnn_data = pad_sequences(input_sequences, maxlen=1200)\n",
    "print(input_cnn_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prediction_0 = model_load_0.predict(input_cnn_data)\n",
    "prediction_1 = model_load_1.predict(input_cnn_data)\n",
    "prediction_2 = model_load_2.predict(input_cnn_data)\n",
    "prediction_3 = model_load_3.predict(input_cnn_data)\n",
    "prediction = model.predict(input_cnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7683431]]\n",
      "[[0.74854684]]\n",
      "[[0.450613]]\n",
      "[[0.2093962]]\n",
      "[[0.2008025]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction_0)\n",
    "print(prediction_1)\n",
    "print(prediction_2)\n",
    "print(prediction_3)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
